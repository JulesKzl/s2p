\documentclass[paper=a4, fontsize=11pt, onecolumn, tikz, dvipsnames, svgnames, x11names]{article}
\input{packages}
\input{macros}

% ------------------------ General informations --------------------------------
\title{\normalfont \normalsize \huge Pointing error correction}
\author{Jules Kozolinsky, Vincent Matthys}
\graphicspath{{images/}{../images/}} % For the images path
% ------------------------------------------------------------------------------


\date{}

\begin{document}
%\maketitle

 \begin{tabularx}{0.9\textwidth}{@{} l X r @{} }
 	{\textsc{Master MVA}}  &  & \textsc{} \\
 	\textsc{Remote sensing} &  & {ENS Paris Saclay}       \\
 \end{tabularx}
 \vspace{1.5cm}
 \begin{center}

 	\rule[11pt]{5cm}{0.5pt}

 	\textbf{\LARGE \textsc{Pointing error correction}}
 	\vspace{0.5cm}

 	Jules Kozolinsky,
 	Vincent Matthys

 	\href{mailto:jules.kozolinsky@ens-cachan.fr}{jules.kozolinsky@ens-cachan.fr} \\
 	\href{mailto:vincent.matthys@ens-paris-saclay.fr}{vincent.matthys@ens-paris-saclay.fr}

 	\rule{5cm}{0.5pt}

 	\vspace{1.5cm}
 \end{center}

 \tableofcontents

\begin{abstract}

\end{abstract}

\section{Attitude of a Satellite}


Roll, pitch and yaw angles. (See Figure \ref{angles}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/angles.jpg}
   \caption{ Roll, pitch and yaw angles.}
   \label{angles}
\end{figure}

\subsection{Localization and projection functions}
Localization and projection functions  allow converting from image coordinates to coordinates
on the globe and back (See Figure \ref{rpc}).


The projection function returns the image coordinates, in pixels, of a given 3-space
point.
\begin{align*}
P:&\; \mathbb{R}^3\to\mathbb{R}^2\\
&(\lambda, \theta, h) \mapsto \textbf{x}
\end{align*}
In that system a point of 3-space is identified by its
longitude $\lambda\in[-180,180]$, latitude $\theta\in[-90,90]$ and
altitude $h$, in meters, above the reference ellipsoid.


The localization function is its
inverse with respect to the first two components. It takes a point $\textbf{x}
= (x, y)^\top$ in the image domain together with an altitude $h$, and
returns the geographic coordinates of the unique 3-space point
$\textbf{X} = (\lambda, \theta, h)$ whose altitude is $h$ and whose image is $\textbf{x}$.
\begin{align*}
L:&\; \mathbb{R}^3\to\mathbb{R}^2\\
&(\textbf{x}, h) \mapsto (\lambda, \theta)
\end{align*}



We use the Rational Polynomial Coefficient (RPC) camera model which is an
analytic description of the projection and localization functions (cite). In the RPC model, the projection and
localization functions are expressed as ratio of multivariate cubic
polynomials.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/rpc_illustration.png}
   \caption{ Projection (RPC) and localization (RPC$^{-1}$) functions.}
   \label{rpc}
\end{figure}

\subsection{Epipolar curves}
For a point $\textbf{x}$ in the image $u$, the epipolar is the curve pararametrized by the alitude $h$ and defined as:
\begin{align*}
\text{epi}_{uv}^{x} : h \rightarrow \text{RPC}_u(\text{RPC}^{-1}_v(\textbf{x}, h))
\end{align*}
where RPC and RPC$^{-1}$ are the projection and localization functions. In pratice we observe this curve as straight parallel lines.

\subsection{Affine fundamental matrix approximation}
From  \cite{de2014automatic}, we know that, for each correspondence $i$, the epipolar curve $\text{epi}^{\textbf{x}_i}_{u v}(R)$ is approximated up to $0.05$ pixels by the straight line $F\textbf{x}_i$, where $F$ is
the affine fundamental matrix between the two views for the considered tile. Since the fundamental matrix, due to the approximation, is \textit{affine}, all the lines $(F\textbf{x}_i)_{i=1...N}$ are parallel.

\section{Satellite attitude error effects on stereo images}
\subsection{Pointing error}
However, due to the limited precision of the
camera calibration, there is a bias of a few pixels in the RPC functions.
For stereo matching, we can not ignore this error since the epipolar constraints are derived from
the parameters of the cameras.
Therefore this bias has to be corrected before applying rectification.\\

This error, often of the order of a few pixels, is the sum, for all correspondences, of the distance between a correspondance on an image and the epipolar curve on the same image computed with the correspondance on the other image.
Formally, given two images $u, v$ and a set of correspondences $(\textbf{x}_i, \textbf{x}'_i)_{i=1...N}$ , the pointing error between $u$ and $v$ is defined by:
\begin{align*}
\dfrac{1}{N} \sum\limits_{i=1}^{N} d(\textbf{x}_i', \text{epi}_{uv}^{\textbf{x}_i})
\end{align*}
which can be approximated, using the affine fundamental matrix $F$, to:
\begin{align*}
\dfrac{1}{N} \sum\limits_{i=1}^{N} d(\textbf{x}_i', F\textbf{x}_i)
\end{align*}




\subsection{Sensibility}
\label{subsec:sensibility}
%Sensiblity of roll, pitch and yaw. Scheme (roll and pitch depend of the height of the satellite whereas yaw only the angle)
The effect of attitude errors on the localization function RPC$^{-1}$ can be computed simply with zero roll and pitch angles. For roll or pitch angles
a small error $\varepsilon$ induces, to first order, a translation of $a\varepsilon$ on the ground, where $a$ is
the distance between the satellite and the ground. For the yaw, a small error $\varepsilon$  induces
a rotation of at most $D\varepsilon/2$ on the ground, where $D$ is the width of the projected pushbroom sensor of the satellite (See Figure \ref{sensiblity}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sensibility.png}
   \caption{Sensibility for roll, pitch and yaw angles.}
   \label{sensiblity}
\end{figure}

For SkySat satellites, we can have an error $\varepsilon \simeq 50 \; \mu \text{rad}$ with leads to an error of about $10$ degree knowing that $D \simeq 9 \text{km}$.
%500 km altitude orbit

\section{Correction of Relative Pointing Error}

A simple and automatic way to correct the pointing error is to transform one of the two images, in such a way that the corresponding points fall on the respective epipolar curves. More formally, given two images $u$, $v$ and a set of correspondences $(\textbf{x}_i , \textbf{x}'_i)_{i=1...N}$ , we search for a transformation $f$ such that, for all $i$, the transformed point $f(\textbf{x}'_i)$ lies on the epipolar curve $\text{epi}^{\textbf{x}_i}_{u v}(R)$.
The desired transformation $f^{*}$ minimises the relative pointing error:
\begin{align}
\label{minif}
f^* = \argmin{f} \dfrac{1}{N} \sum\limits_{i=1}^{N} d(f(\textbf{x}'_i), \text{epi}^{\textbf{x}_i}_{u v}(R))
\end{align}

\paragraph{Affine fundamental matrix approximation\\}
From  \cite{carlo_2014_pushbroom}, we know that, for each correspondence $i$, the epipolar curve $\text{epi}^{\textbf{x}_i}_{u v}(R)$ is approximated up to $0.05$ pixels by the straight line $F\textbf{x}_i$, where $F$ is
the affine fundamental matrix between the two views for the considered tile. Since the fundamental matrix, due to the approximation, is \textit{affine}, all the lines $(F\textbf{x}_i)_{i=1...N}$ are parallel.

\subsection{Roll and Pitch Angles}
Because of sensitivity issues, we first can take only roll and pitch error into account. Therefore, according to section \ref{subsec:sensibility}, we search for a transformation $f$ such that
\begin{align*}
f(\textbf{x}) = T\textbf{x}
\end{align*}
where $T$ is a translation:
\begin{align*}
T =
\begin{pmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{pmatrix}
\end{align*}
We can write the transformation $f$, for $ \textbf{x} = (  x \; y \; 1)^T $, as:
\begin{align*}
f(\textbf{x}) = T\textbf{x} =
\begin{pmatrix}
x + t_1 \\
y + t_2 \\
1
\end{pmatrix}
\end{align*}


Without any additional restriction, we may assume that these lines are horizontal (otherwise just do a change of coordinates). This change of coordinates is called \textit{rectification}. We find \\%TODO explain rectification

After rectification, for each point $i$, the horizontal line $F\textbf{x}_i$ can be written as
\begin{align*}
F\textbf{x}_i = \left[ 0 \; 1 \; c_i \right]
\end{align*}
With these notations, for each point correspondence $(\textbf{x}_i , \textbf{x}'_i)$, the pointing error $e$ is:
\begin{align*}
e(\textbf{x}_i, \textbf{x}'_i) = d(\textbf{x}'_i, epi^{\textbf{x}_i}_{u v}(R)) = d(\textbf{x}'_i, F\textbf{x}_i) = | y'_i + c_i|
\end{align*}

Here the error $e$ is invariant to any horizontal translation, thus the search for a translation minimizing the relative pointing error of formula (\ref{minif}) can be restricted to vertical translations only. With a vertical translation of parameter $t$, the global pointing error becomes
\begin{align*}
E = \dfrac{1}{N} \sum\limits_{i=1}^{N} d(T\textbf{x}'_i, F\textbf{x}_i) = \dfrac{1}{N} \sum\limits_{i=1}^{N} | y'_i + t+ c_i|
\end{align*}
The translation that minimizes this sum is given by the geometric median (Weiszfeld, 1937) of the vector $(-y'_i - c_i )_{i=1...N}$.  The pointing error can thus be minimized by applying a translation to one of the images. Note that the median is robust against outliers, thus this correction procedure works well even in the presence of false matches.


\subsection{Roll, Pitch and Yaw Angles}
If we assume that the scene is located at infinity with respect to the satellite, an error in the sensor attitude measurement can be modeled in image space as a translation composed with a rotation. Therefore we have
\begin{align*}
f(\textbf{x}) = TR\textbf{x}
\end{align*}
where $R$ is a rotation and $T$ a translation:\\
\begin{align*}
R =
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) & 0 \\
\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 1
\end{pmatrix}, \;
T =
\begin{pmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{pmatrix}
\end{align*}

\subsubsection{Approximation of the Rotation}

To correct the pointing error in the rectified setting we find $R^*$ and $T^*$ minimizing the pointing error defined as follow :
\begin{align}
    (R^*, T^*) = \argmin{R,T} \sum_{i=1}^N d(TR\bm{x_i'}, F\bm{x}_i)^2
\end{align}
As in~\cite{carlo_2014_automatic}

After rectification, the horizontal line $F\bm{x}_i$ can be written, in homogeneous coordinates, as
\begin{align*}
F\bm{x}_i =  \begin{bmatrix} 0 & 1 & y_i \end{bmatrix}
\end{align*}
With these notations, if the model now includes a rotation $R$, for each point correspondence $(\bm{x}_i , \bm{x}'_i)$, we have
\begin{align*}
e(\textbf{x}_i, \textbf{x}'_i) = d(TR\bm{x}'_i, \text{epi}^{\bm{x}_i}_{u v}(R)) = d(TR\bm{x}'_i, F\bm{x}_i) = |x_i' \sin \theta   + y_i' \cos \theta  + t - y_i |
\end{align*}
where $\theta$ is the angle of the rotation.

We can consider that $\theta$ is small enough compare to $2\pi$. In fact on satellites such as Sentinel or Pleiades (%TODO source)
the precision of the yaw angle is around $50~\mu \text{rad}$. Even if Planet's sensor would be $100$ less precise, we will still have an yaw angle less than $1$ degree.
With this approximation, we can write:
\begin{align*}
e(\textbf{x}_i, \textbf{x}'_i) = |x_i' \theta   + y_i' + t - y_i |
\end{align*}

Correcting the global pointing error is to minimize:
\begin{align*}
(\theta^*, t^*) = \argmin{\theta,t} \sum_{i=1}^N d(TR\bm{x_i'}, F\bm{x}_i)^2 = \sum_{i=1}^N (x_i' \theta   + y_i' + t - y_i)^2
\end{align*}
We can write this optimization problem as :
\begin{align*}
X^* = \argmin{X} \| AX + b \|^2
\end{align*}
where
\begin{align*}
A =
\begin{pmatrix}
x_1' & 1 \\
x_2' & 1 \\
\vdots & \vdots \\
x_n' & 1 \\
\end{pmatrix} \quad
X =
\begin{pmatrix}
\theta \\
t
\end{pmatrix} \quad
b =
\begin{pmatrix}
y_1' - y_1\\
y_2' - y_2\\
\vdots\\
y_1' - y_1\\
\end{pmatrix}
\end{align*}

The solution of this optimization problem is the solution of the normal equation
\begin{align*}
 A^TAX = -A^Tb
\end{align*}
Here,
\begin{align*}
A^TA =
\begin{pmatrix}
\sum\limits_{i=1}^N x_{i}'^2 & \sum\limits_{i=1}^Nx_{i}' \\
\sum\limits_{i=1}^N x_{i}' & N \\
\end{pmatrix}
\end{align*}
So $A^TA$ is inversible, and the optimal correction in the rectified images is:
\begin{align*}
\begin{pmatrix}
\theta^* \\
t^*
\end{pmatrix} = X^* = -(A^TA)^{-1}A^Tb
\end{align*}


\subsubsection{Solving Numerically an Optimization Problem}
To correct the pointing error in the rectified setting we find $R^*$ and $T^*$ minimizing the pointing error defined as follow :
\begin{align}
    (R^*, T^*) = \argmin{R,T} \sum_{i=1}^N d(RT\bm{x_i'}, F\bm{x}_i)
\end{align}

Since we are not able to solve this optimization problem analytically, we solve it numerically thanks to the L-BFGS algorithm. As input are the correspondences on the images and the affine approximation matrix and the optimization algorithm outputs the optimal angle $\theta \in [0, 2\pi]$ corresponding to the error on the yaw and a translation $t \in \mathbb{R}$ corresponding to a translation in the normal direction w.r.t. the epipolar lines.

\paragraph{Limited-memory BFGS\\}
Limited-memory BFGS is an optimization algorithm in the family of quasi-Newton methods that approximates the BFGS algorithm using a limited amount of computer memory. In our case we do not need to compute the gradient analytically since the algorithm approximates it using finite difference. We use this algorithm as a black box to solve the minimization problem.

\section{Data overview}

\subsection{Test data}

S2P pipeline comes with a pair and a triplet of test data captured by Pleiades. The input pair and their geolocalisation is shown below in figure~\ref{fig_geoloc_testdata}. After the pair, the triplet is also represented in figure~\ref{fig_geoloc_testdata_triplet}. The expected output result is also shown in figure~\ref{fig_expected_output_pair}, and is offline visuable using the PotreeConverter module available on github~\cite{PotreeConverter}.

\begin{figure}[H]
    \centering
    \includegraphics[height = 0.4\textheight]{elevation_intro.png}
    \caption{Expected output of S2P on test data input pair}
    \label{fig_expected_output_pair}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width = 0.95\textwidth]{ROI_testdata_pair.jpeg}
        \caption{Region of intereset (ROI, in red) in test data input pair}
    \end{subfigure}%
    ~
    \vfill
    ~
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width = 0.95\textwidth]{geoloc_testdata.png}
        \caption{Geolocalisation of the test data input pair, in La RÃ©union. Reprojected using an elevation of $h=2350~m$. Center of ROI $(lat, lon) ~\approx (-21.23066754, 55.65022517)$}
    \end{subfigure}%
    \caption{Overview of the test data input pair}
    \label{fig_geoloc_testdata}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width = 0.95\textwidth]{ROI_testdata_triplets.png}
        \caption{Region of intereset (ROI, in red) in test data input triplet. Only one (img\_01.tif) of the 3 images is represented.}
    \end{subfigure}%
    ~
    \vfill
    ~
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width = 0.95\textwidth]{geoloc_testdata_triplets.png}
        \caption{Geolocalisation of the test data input triplet, east of Marseille. Reprojected using an elevation of $h=300~m$. Center of ROI $(lat, lon) ~\approx (43.26182762, 5.443071)$}
    \end{subfigure}%
    \caption{Overview of the test data input triplet}
    \label{fig_geoloc_testdata_triplet}
\end{figure}

\subsection{Planet Data}

Data provided are coming from Skysat-2 and Skysat-3 which are made of 3 camera each, with a revisit time of $3$ to $4$ day at $500~km$ altitude~\cite{planet_product}. The agility of Skysat satellite is 2.3 target (6.6 x 10 km) per minute. The resolution of the panchromatic images is $0.9~m$, and academics can apply to their Education and Research program to download up to $10000~km^2/month$ of those data.

\subsubsection{Structure}

The structure of the provided Skysat-2 data are represented in figure~\ref{fig_forest_skysat_2}. Each trace is identified with a catalog ID, for example s03\_20161003T161107Z, which uniquely identifies the trace for s03 (Skysat-3) timed at $16:11:07$ Zoulou the $03/10/2016$. For each trace, the panchromatic and the pansharp data are available; nonetheless, we only used the panchromatic ones. The 3 detectors of skysat are then identified with $d1$, $d2$ and $d3$, and images from each detector captured during the trace are stored as a trip of single frames, with a small horizontal and vertical overlap. With each tif image file comes the rpc model stored in ikonos convention. Each image is numbered between $1$ and $18$ for each detector, and the common POSIX time for a number for the three detectors is stored in the RPC file associated with the take. For 18 images, for the 3 detectors, the necessary time is approximately $9.2~s$.

The structure of the provided Skysat-3 data are simpler: 892 panchromatic frames were collected from $02:05:54$ Zoulou to $02:06:23$ Zoulou the $07/05/2015$

\begin{figure}
    \centering
    \resizebox{!}{.7\textheight}{
    \begin{forest}
      my label/.style={
        label={[font=\sffamily]right:{#1}},
      },
      for tree={
        folder,
        font=\sffamily,
        text=white,
        minimum height=0.75cm,
        if level=0{fill=ForestGreen}{fill/.wrap pgfmath arg={SlateBlue#1}{int(4-(mod((level()-1),4)))}},
        rounded corners=4pt,
        grow'=0,
        edge={ForestGreen,rounded corners,line width=1pt},
        fit=band,
      },
      [data
        [s03\_20161003T161107Z
          [panchromatic
            [s03\_20161003T161107Z\_pan\_d1\_0001\_rpc.txt]
            [s03\_20161003T161107Z\_pan\_d1\_0001.tif]
          ]
          [pansharp
            [s03\_20161003T161107Z\_pansharp\_bgrn\_d1\_0001\_rpc.txt]
            [s03\_20161003T161107Z\_pansharp\_bgrn\_d1\_0001.tif]
          ]
        ]
        [s03\_20161003T161148Z
          [panchromatic
            [s03\_20161003T161148Z\_pan\_d1\_0001\_rpc.txt]
            [s03\_20161003T161148Z\_pan\_d1\_0001.tif]
          ]
          [pansharp
            [s03\_20161003T161148Z\_pansharp\_bgrn\_d1\_0001\_rpc.txt]
            [s03\_20161003T161148Z\_pansharp\_bgrn\_d1\_0001.tif]
          ]
        ]
        [s03\_20161003T161231Z
          [panchromatic
            [s03\_20161003T161231Z\_pan\_d1\_000\_rpc.txt]
            [s03\_20161003T161231Z\_pan\_d1\_0001.tif]
          ]
          [pansharp
            [s03\_20161003T161231Z\_pansharp\_bgrn\_d1\_0001\_rpc.txt]
            [s03\_20161003T161231Z\_pansharp\_bgrn\_d1\_0001.tif]
          ]
        ]
      ]
  \end{forest}
  }

    \caption{Structure of the provided Skysat-2 data}
    \label{fig_forest_skysat_2}
\end{figure}

\subsubsection{Geolocalisation}

The geolocalisation have been done using the rpc model, with a reprojection altitude depending of the area considered: for Skysat-2, near Cushing (United States), an altitude of $h=285~m$ (mean altitude of Cushing) has been used, while an altitude of $h=0m$ has been used for Skysat-3 video, in port Hedland (Australia). The three traces for Skysat-2 are shown in chronological order in figure~\ref{fig_trace_7Z}~\ref{fig_trace_8Z}~\ref{fig_trace_1Z}, while the trace for Skysat-3 only represents the 1t, 300th, and 600th images in the video.

\newpage
\begin{figure}
    \centering
    \includegraphics[height = 0.8\textheight]{trace_7Z.png}
    \caption{Trace s03\_20161003T161107Z}
    \label{fig_trace_7Z}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[height = 0.8\textheight]{trace_8Z.png}
    \caption{Trace s03\_20161003T161148Z}
    \label{fig_trace_8Z}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[height = 0.8\textheight]{trace_1Z.png}
    \caption{Trace s03\_20161003T161231Z}
    \label{fig_trace_1Z}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width = 0.8\textwidth]{trace_video.png}
    \caption{Trace s02\_20150507T020554Z}
    \label{fig_trace_video}
\end{figure}

\subsection{Mosaic of data}

\newpage
\begin{figure}
    \centering
    \includegraphics[width = 0.95\textwidth]{d1.png}
    \caption{Mosaic of d1. From left to right: 1Z, 8Z, 7Z}
    \label{fig_mosaic_d1}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width = 0.95\textwidth]{d2.png}
    \caption{Mosaic of d2. From left to right: 1Z, 8Z, 7Z}
    \label{fig_mosaic_d2}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width = 0.95\textwidth]{d3.png}
    \caption{Mosaic of d3. From left to right: 1Z, 8Z, 7Z}
    \label{fig_mosaic_d3}
\end{figure}


\appendix

\section{Rotations and translation}

In this part, we consider any point \((x, y) \in \mathbb{R}^2\). As translation and rotation do not commute, we will consider two cases: rotation followed by translation and translation followed by rotation, with \(R\) denoting the rotation of angle \(\theta\) and \(T\) denoting the translation of vector \((t_x, t_y)\). Naming \(f\) the resulting function.

\begin{align*}
    T &=
    \begin{pmatrix}
    1 & 0 & t_x \\
    0 & 1 & t_y \\
    0 & 0 & 1
    \end{pmatrix}
    \\
    R &=
    \begin{pmatrix}
    \cos \theta & -\sin \theta & 0 \\
    \sin \theta & \cos \theta & 0 \\
    0 & 0 & 1
    \end{pmatrix}
\end{align*}


\subsection{Rotation followed by a translation}
This happens when correcting the pointing error first with the rotation.

\begin{align*}
    f(\bm{x}) &= TR\bm{x} \\
    &=
    \begin{pmatrix}
    1 & 0 & t_x \\
    0 & 1 & t_y \\
    0 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
    \cos \theta x - \sin \theta y \\
    \sin \theta x + \cos \theta y \\
    1
    \end{pmatrix}
    \\
    &=
    \begin{pmatrix}
    \cos \theta x - \sin \theta y + t_x\\
    \sin \theta x + \cos \theta y + t_y\\
    1
    \end{pmatrix}
\end{align*}

\subsection{Translation followed by a rotation}
This happens when correcting the pointing error first with the translation

\begin{align*}
    f(\bm{x}) &= RT\bm{x} \\
    &=
    \begin{pmatrix}
    \cos \theta & -\sin \theta & 0 \\
    \sin \theta & \cos \theta & 0 \\
    0 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
    x + t_x \\
    y + t_y \\
    1
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
    \cos \theta (x + t_x) - \sin \theta (y + t_y)\\
    \sin \theta (x + t_x) + \cos \theta (y + t_y)\\
    1
    \end{pmatrix}
\end{align*}


\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

\end{document}
